{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0e86c4-aba1-4adb-beb3-b35f21fbfcc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b438a77c-41e3-4010-b6b4-76f5d3131288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define your config\n",
    "storage_account_name = \"adfprojectstorage\"\n",
    "container_name = \"bronze\"\n",
    "relative_path = \"Report (17).csv\"\n",
    "sas_token = \"sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-05-31T13:27:04Z&st=2025-04-30T05:27:04Z&spr=https&sig=ju1VNkjB6Wu2JBGwu4L5T7t5MnhJjP0fmH6zvi7EfxY%3D\"\n",
    "\n",
    "# Set the Spark config to use the SAS token\n",
    "spark.conf.set(f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\", sas_token)\n",
    "\n",
    "# Use wasbs:// (not abfss://) for SAS\n",
    "wasbs_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{relative_path}\"\n",
    "\n",
    "# Read the CSV file\n",
    "raw_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(wasbs_path)\n",
    "display(raw_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea55897-8500-458f-902b-d1af0af0ea94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_timestamp, expr, lpad,date_format\n",
    "\n",
    "select_specfic_col_df = raw_df.select(col(\"Fault Number\"),col(\"short Description\"),col(\"Assignee Group\"),col(\"submitter\"),col(\"Create Date\"),col(\"Service Outage Start Date/Time\"))\n",
    "display(select_specfic_col_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590502d7-4216-4f0f-aa7c-01dd35702b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Convert string columns to timestamp\n",
    "select_specfic_col_df = select_specfic_col_df.withColumn(\n",
    "    \"Create_Date\", to_timestamp(col(\"Create Date\"), \"dd/MM/yyyy HH:mm\")\n",
    ").withColumn(\n",
    "    \"Service_Outage_Start\", to_timestamp(col(\"Service Outage Start Date/Time\"), \"dd/MM/yyyy HH:mm\")\n",
    ")\n",
    "\n",
    "select_specfic_col_df = select_specfic_col_df.withColumn(\n",
    "    \"Create_Date_dd/MM/yyyy\", date_format(col(\"Create_Date\"), \"dd/MM/yyyy HH:mm\")\n",
    ").withColumn(\n",
    "    \"Service_Outage_Start_dd/MM/yyyy\", date_format(col(\"Service_Outage_Start\"), \"dd/MM/yyyy HH:mm\")\n",
    ")\n",
    "\n",
    "# Step 2: Calculate difference in seconds\n",
    "select_specfic_col_df = select_specfic_col_df.withColumn(\n",
    "    \"alarm_seconds\",\n",
    "    unix_timestamp(col(\"Create_Date\")) - unix_timestamp(col(\"Service_Outage_Start\"))\n",
    ")\n",
    "\n",
    "# Step 3: Format seconds into HH:mm:ss\n",
    "select_specfic_col_df = select_specfic_col_df.withColumn(\n",
    "    \"MTTD\",\n",
    "    expr(\"\"\"\n",
    "        concat(\n",
    "            lpad(floor(alarm_seconds / 3600), 2, '0'), ':',\n",
    "            lpad(floor((alarm_seconds % 3600) / 60), 2, '0'), ':',\n",
    "            lpad(alarm_seconds % 60, 2, '0')\n",
    "        )'\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# Step 4: Display the result\n",
    "display(select_specfic_col_df.select(\"Fault Number\",\"short Description\",\"Assignee Group\",\"submitter\",\"Create Date\", \"Create_Date_dd/MM/yyyy\",'Create_Date',\"Service Outage Start Date/Time\", \"Service_Outage_Start_dd/MM/yyyy\",'Service_Outage_Start',\"alarm_seconds\",\"MTTD\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6eed3a-879a-4e91-9313-85dd9ee53eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = select_specfic_col_df.select(\"Fault Number\",\"short Description\",\"Assignee Group\",\"submitter\", \"Create_Date_dd/MM/yyyy\", \"Service_Outage_Start_dd/MM/yyyy\",\"MTTD\")\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62858346-9ad6-4a6b-bc46-5c639329d476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project_MTDR",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}